A **closed-form** expression isÂ one that is **composed of finite number of operations**.

When linear regression is used but observations are correlated (as in **time series** data) you will have a biased estimate of the variance.

If there are many input variables, use **gradient descent** to find the line of best fit
> Gradient descent is a technique for optimization that involves taking the gradient at each point and moving a small step (the learning rate) in that direction
> gradient of the MSE loss function

**Gradient descent** is a technique for optimization that involves taking the gradient at each point and moving a small step (the **learning** rate) in that direction.

As the error decreases, **decrease** the step size.
> if the learning rate is too high, the algorithm will move back and forth forever
> too low and it will take too long
> ![](z_attachments/Pasted%20image%2020250408142727.png)

**Stochastic** gradient descent is the same as gradient descent but **at each time a random subset of the points is used**.

The random subset in stochastic gradient descent is known as the **batch size**.

For linear correlation, the data must be **homoscedastic**, meaning **the variance in the outliers must be constant**.

Polynomial regression is prone to **overfitting**; generally only use it for **small degree polynomials**. 
+
**Regularization** helps polynomial equations not overfit: adds term to loss function penalizing large weights

**Ridge** regression does well in situations with multicollinearity
> where two independent variables are correlated.
> Ridge regression retains all features in the model, reducing the impact of less important features by shrinking their coefficients.	

**Lasso** regression is useful when there are large numbers of potentially irrelevant features. Applies penalty to prevent overfitting.
> Lasso regression can set some coefficients to zero, effectively selecting the most relevant features and improving model interpretability.

**ElasticNet** regression combines loss functions of ridge and lasso regression to get advantages of both
> popular interview question lol

Random forest regression predicts regression using **line segments**.

Root mean squared error is the actual loss function.
It's good at comparing two models for the same data but **is in the same units as the data**, so it's useless on its own.

***
