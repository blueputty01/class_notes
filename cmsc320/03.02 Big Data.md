3 V's (components) of big data: **volume**, **1::velocity**, **1::variety**

---

volume: vast amount of data generated or collected; this has increased exponentially

velocity: represents speed at which data is generated

---

# Hadoop

Hadoop is a system for **data storage, almost like a file system**
> It is designed to look, from the outside, very similar to the typical unix filesystem.

Hadoop Structure
- **Name** nodes route traffic throughout the the file system
- **1::Data** Nodes store the actual data

The **NameNode** is the main point of access of a Hadoop cluster.
> responsible for bookkeeping of the data partitioned across the data nodes
> manages filesystem metadata 
> performs load balancing

The **Secondary NameNode** keeps track of changes in the NameNode
> performs regular snapshots, allowing quick startup
> guaranteed high availability (since NameNode is single point of failure)

In HDFS (Hadoop File System), data is split into **blocks**.
> so all block objects have the same size

Each DataNode sends a **heartbeat** signal to the NameNode periodically as a sign of vitality.
> Whenever a DataNode becomes unavailable (due to network or hardware failure), the NameNode stops sending requests to that node and creates new replicas of the blocks stored on that node

HDFS follow the **write once read many** approach for files, assuming that a file in HDFS will not be modified once written. This enables high **throughput data access** and simplifies **2::data coherency issues**.
> web crawler/mapreduce application best suited for HDFS
> focus on how to retrieve data at fastest possible speed
> reading complete data more important than accessing a single record
> overlooks a few POSIX requirements to implement streaming data access
> HDFS has strictly one writer

# Spark

Spark is an open source data analytics engine that can process massive streams of data from **multiple** sources.

**MapReduce** is a programming model that uses **parallel** processing to speed large scale data processing; it enables massive scalability across hundreds or thousands of servers within a Hadoop cluster.

MapReduce steps:
1. Input
2. **Splitting**
3. **1::Mapping: convert data into key value pairs**
4. **1::Shuffling: sort map outputs and assign all key/value pairs with the same key to the same reducer**
5. **1::Reducing**
6. Result
> MapReduce essential functions:
> 1 map, which converts data into key/value pairs
> 2 reduce, which aggregates all values with same key and processes the data to produce a final set of key/value pairs
> example: find maximum temperature for each city across a time period (in general, data can be structured or unstructured)
> ![](z_attachments/Pasted%20image%2020250424155917.png)
 
The **SparkContext** is the main entry point for Spark functionality that then communicates with **Cluster Managers**

The first step in using Apache Spark is to start a **SparkSession**

Data in Apache Spark is represented by **data frames**

Iterating over rows in a Spark dataframe is **impossible::discouraged/ok**
> Hadoop may be keeping them in different places

Apache Spark Dataframes are **immutable::mutable/immutable**

Spark operations are **lazy::lazy/eagar** evaluated
> transformations happen in memory

***
