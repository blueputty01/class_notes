A model is a mathematical object that takes data where the label is **unknown** and **1::assigns it a label**.

A **weighted KNN** model is a model that assigns a label to a data point by taking a weighted average of the labels of the k-nearest neighbors of that data point.
	$y=\sum_{k\in K}\frac{1}{dist(k,p)}*k_{label}$
	![](z_attachments/Pasted%20image%2020250325141324.png)
	must be able to store the entire training set (peto-bytes or exo-bytes)
	$K$ must be tuned correctly

In a weighted KNN model, searching for nearest neighbors is time consuming (this can be helped by **spherical** approach through a K-D tree)

In the KNN model, if $K=N$, we will predict the **mode of the data**.

KNN is a(n) **instance based** model meaning it requires all the training data be stored to work.

A **hyperparameter** is an argument to the model that determines how the model behaves.
	eg $K$ in an **KNN** model

***

# Decision Trees

**Classification** algorithm: 
get a new piece of data with no label
start at the top of the tree
god down each branch based on appropriate feature
when you get to a leaf, classify

Supervised learning models have two phases:
- Training: where the model is learned
- **Classification: where the model takes unlabeled data and provides a label for it**
> KNN doesn't require training

Machine learning models are powerful because they **generalize**.
> less parameters may be better
> Considering the first 4/5 parameters gets you 70% of the data; one more gives you like 1% benefit

**Entropy** is a measure of how much information something provides. 
Measured in **bits**
> one bit of entropy is the amount of information it takes to encode a yes/no signal
> eg: result of two coin flips is two bits, three flips is three bits

The formula for entropy is **$\mathrm{H}(X)=-\sum_{x\in\mathcal{X}}p(x)\log_bp(x)$**
When $p$ is close to $1$, the surprisal of the event is **low**
> $b=2$ for bit entropy
> where $p$ is the proportion of + vs -
> log base 2 because when doubling the number of outcomes the number of bits should increase by 1

Algorithm for picking a split:
Start with all the data and no split
**Try each feature**
Compute the difference between current entropy and new entropy (weighted average of split)
Select feature causing leaves to be most pure
> children should have lower entropy

Decision trees are used for a pretty picture and **are not::are/are not** used in real life

Decision trees **may not::may/may not** always produce the optimal tree.



***
