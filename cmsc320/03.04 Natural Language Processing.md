# topic modelling

A **corpus** is a collection of text or speech. It can be a single document or a collection.
> For example the Brown corpus is a million-word collection of samples from 500 written English texts from different genres (newspaper, fiction, non-fiction, academic, etc.), assembled at Brown University in 1963–64

A **bag of words** is an unordered set of words with their position ignored, keeping only their **frequency** in the document.

**Topic modeling** is a type of model for discovering the topics that occur in a collection of documents.

Latent semantic analysis (LSA) performs singular-value-decomposition on the **document-term** matrix and compares documents using **cosine similarity**
> also known as latent semantic indexing

**Cosine** similarity defines the angle between two vectors in vector space.
> $\cos(\theta)=\frac{\mathbf{A}\cdot\mathbf{B}}{\|\mathbf{A}\|\|\mathbf{B}\|}=\frac{\sum_{i=1}^nA_iB_i}{\sqrt{\sum_{i=1}^nA_i^2}\sqrt{\sum_{i=1}^nB_i^2}}$

# preprocessing

**Stemming** is the process of reducing inflected form of a word into the "stem" or root form. **1::Lemmatization** ensures the output word is an existing normalized form of the word that can be found in the dictionary.
> also known as lemma in linguistics
> It is one of two primary methods—the other being [lemmatization](https://www.ibm.com/topics/stemming-lemmatization)—that reduces inflectional variants within a text dataset to one morphological lexeme.
> Shrink dimensionality of ML algorithms
> How do you decide which part to truncate?
> [What Is Stemming? \| IBM](https://www.ibm.com/think/topics/stemming)

**Stop words** are words that are filtered out during the NLP pipeline since they have little semantic value.
> **Stop words** are the words in a **stop list** (or _**stoplist**_ or _**negative dictionary**_) which are filtered out ("stopped") before or after [processing of natural language](https://en.wikipedia.org/wiki/Natural_language_processing "Natural language processing") data (i.e. text) because they are deemed to have little semantic value or are otherwise insignificant for the task at hand.[1](https://en.wikipedia.org/wiki/Stop_word#cite_note-1) There is no single universal list of stop words used by all natural language processing (NLP) tools, nor any agreed upon rules for identifying stop words, and indeed not all tools even use such a list. Therefore, any group of words can be chosen as the stop words for a given purpose. The "general trend in [information retrieval] systems over time has been from standard use of quite large stop lists (200–300 terms) to very small stop lists (7–12 terms) to no stop list whatsoever". [2](https://en.wikipedia.org/wiki/Stop_word#cite_note-2)

A(n) **n-gram** is a collection of $n$ successive items in a text document that may include words, numbers, symbols, and punctuation.

# word embeddings

**Word embeddings** are a way of representing words as vectors in a multi-dimensional space, where words that are **closer** in the space are expected to be similar in meaning.
> Traditional methods of representing words in a way that machines can understand, such as one-hot encoding, represent each word as a sparse vector with a dimension equal to the size of the vocabulary. Here, only one element of the vector is "hot" (set to 1) to indicate the presence of that word. While simple, this approach suffers from the curse of dimensionality, lacks semantic information and doesn't capture relationships between words.

Raw frequency of words across documents is a bad representation of word-word similarity since **polysemy (one word having multiple meanings)** and **1::homonymy (accidental similarity between two or more words; eg bear (animal) and bear (verb))** are not handled properly. 

---

Raw word count vectors don't work very well. Issues:

- The raw counts require normalization.
- Vectors are too long and too sparse.

Why do the counts need normalization?

- rare words have fewer observations than common ones and, similarly, short documents have fewer words than longer ones
- very frequent words aren't useful context
- local observations (e.g. within a document) are highly correlated
- rare words provide unreliable context

So we'll need to

- normalize the counts
- map the vectors into a lower-dimensional space

---

Two older methods to normalize word counts and map the vectors onto a lower-dimensional space:
- **TF-IDF**
- **1::PMI (Pointwise mutual information): modeling a word's meaning on the basis of words seen near it**
> [CS440 Lectures](https://courses.grainger.illinois.edu/cs440/fa2018/lectures/lect36.html)

The TF-IDF feature is the product of TF (**normalized word count::what is it**) and IDF (**inverse document frequency::what is it**).
+
The document frequency ($DF$) of the word is $df/N$, where $N$ is the total number of documents and $df$ is the number of documents our word appears in. 
When $DF$ is small, our word provides **a lot of** information about the topic. 
When $DF$ is large, our word provides **3::little (as it is used in a lot of different contexts)** information about the topic.

---

To compute TF, we smooth the raw count c for the word.

>$TF=1+\log_{10}(c)$, if count > 0  
> $TF=0$, if count = 0

- TF does not account for the global importance of a term across the entire corpus.
- Common words like “the” or “and” may have high $TF$ scores but are not meaningful in distinguishing documents.

The document frequency ($df$) of the word, is $df/N$, where $N$ is the total number of documents and $df$ is the number of documents our word appears in. When $df$ is small, our word provides a lot of information about the topic. When $df$ is large, our word is used in a lot of different contexts and so provides little information about the topic.

The normalizing factor IDF is computed as

> $IDF=\log_{10}(\frac{N}{df})$

That is, we invert DF and estimate the importance of the numbers using a log scale. This is partly motivated by the fact that human perceptions of intensity or quantity are often well modelled by a log scale.

$w_{t,d}=\mathrm{tf}_{t,d}\times\mathrm{idf}_t$

[CS440 Lectures](https://courses.grainger.illinois.edu/cs440/fa2018/lectures/lect36.html)

![|450](z_attachments/Pasted%20image%2020250429175345.png)

---

`word2vec` directly learns how to embed words into an n-dimensional feature space.
It leverages two models, **Continuous Bag of Words (CBOW)** and **1::Continuous Skip-gram**.
+
General approach: instead of counting how often each word $w$ appears near target:
- **3::train a classifier on a binary prediction task (is $w$ likely to show up near target?)**
- **3::take the learned classifier weights as word embeddings**
This results in self-supervised learning
+
The goal of **CBOW** is to predict a target word given the context words in a sentence. This is in contrast to the **2::skip-gram** model, which predicts context words given a target word.
> [web.stanford.edu/\~jurafsky/slp3/slides/vectorsemantics2024.pdf](https://web.stanford.edu/~jurafsky/slp3/slides/vectorsemantics2024.pdf)
> CBOW:
> ![|300](z_attachments/Pasted%20image%2020250429181500.png)
> +
> skip-gram:
> ![](z_attachments/Pasted%20image%2020250429181517.png)

Properties of embeddings:
- The **kinds of neighbors** depend on window size
- **Analogical** relations through the parallelogram method (but only for frequent words, small distances, and certain relations--open area of research)
Observe:
- historical semantics shift (eg gay: happy to homosexual)
- cultural bias (father:doctor :: mother:nurse)

---

Small windows (C= +/- 2): nearest words are syntactically similar words in same taxonomy
- Hogwarts nearest neighbors are other fictional schools (Sunnydale, Evernight, Blandings)
Large windows (C= +/- 5): nearest words are related words in same semantic field
- Hogwarts nearest neighbors are Harry Potter world (Dumbledore, half-blood, Malfoy)

![](z_attachments/Pasted%20image%2020250429182008.png)

![](z_attachments/Pasted%20image%2020250429182016.png)

---

***

**Embedding** is a means of representing objects like text, images and audio as **points in a continuous vector space**.
***
