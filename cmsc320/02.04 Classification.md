naive bayes
> often used in NLP
> works best if variables are conditionally independent

Support vector machines divide data based on the maximum margin hyperplane. 
If the data is not linearly separable , it uses cool math to introduce new dimensions that slice the data

Random forests: probably the most powerful classifier we have right now
> creates n different decision trees, each trained on a subset of the data to avoid overfitting
> Each decision tree votes
> If a decision tree is overfit or has locked on to a weird correlation, it gets outvoted
> Not great for super high-dimensional data
> ![](z_attachments/Pasted%20image%2020250403142810.png)



Scikit learn has a classifier super object with the methods:
- **`fit(data, labels)`**: A function that trains the model
- **`predict(unlabelled_data)`**: Takes in unlabeled data and produces an array of labels

How Max debugs dataframes:
- A dataframe of random data, completely uncorrelated from my target
> should be no better than random guessing

How Max debugs dataframes:
- A dataframe that has exactly the same data as my regular data, except an extra column that is my target
> what does he mean by 100% target?
> should get 100%

How Max debugs dataframes:
- A dataframe where I overwrite the target to be a really simple function of my existing data
> eg it should pick up on doubling age producing doubling age 

To deal with class imbalance: 
- **use the `class_weight` parameter in Sklearn**
- **create multiple examples of negative features with slight differences**
- equalize your training set 
