The Naive Bayes rule classifier predicts **probability of class given features**
> often used in NLP
> works best if variables are conditionally independent

**Support vector machine** is a numerical method to divide data based on the maximum margin hyperplane. 
	If the data is not linearly separable, it uses cool math to introduce new dimensions that slice the data

Neural networks are best on **unstructured** data.

**Random forests** are probably the most powerful classifier we have right now
> creates n different decision trees, each trained on a subset of the data to avoid overfitting
> Each decision tree votes
> If a decision tree is overfit or has locked on to a weird correlation, it gets outvoted
> Not great for super high-dimensional data
> ![](z_attachments/Pasted%20image%2020250403142810.png)


A **random forest** creates **$n$ different decision trees**, each trained on **a subset of the data** to avoid overfitting. **Voting** occurs after.
> Random forests are probably the most powerful classifier we have right now
> Not great for super high-dimensional data
> ![](z_attachments/Pasted%20image%2020250403142810.png)

Random forests aren't great for high-**dimensional** data

Scikit learn has a classifier super object with the methods:
- **`fit(data, labels)`**: A function that trains the model
- **`predict(unlabelled_data)`**: Takes in unlabeled data and produces an array of labels

How Max debugs dataframes:
- A dataframe of **random data, completely uncorrelated from my target**
- A dataframe that **has exactly the same data as my regular data, except an extra column that is my target**
- A dataframe where **I overwrite the target to be a really simple function of my existing data**
> should be no better than random guessing
> should get 100%
> eg it should pick up on doubling age producing doubling age 

To deal with class imbalance: 
- **use the `class_weight` parameter in Sklearn**
- **create multiple examples of negative features with slight differences**
- equalize your training set 

***
