Ten Fold Cross Validation steps: 
- **break the data into ten train/test splits**
- **train on those, report test accuracy**

Leave One Out Cross Validation steps: 
train on **single sample**
will let you see where algorithm fails
useful when dataset is **small**
> over 500 may be too many

**Overfitting** is when the model becomes too tuned on the training set and becomes unable to generalize.
> memorizing definitions or numbers

A standard method for addressing **overfitting** in decision trees is **pruning**: cut portions of the tree, preventing it from expanding to its maximum depth
> between 3 and 7

A model is **underfit** when the accuracy is low on the dataset
> when $k=n$

**Spurious** correlations increase when features increase; also chance of **overfitting**.
> but maybe glasses do matter!
> but usually, things that don't matter usually don't matter

**Class imbalance** is when an overwhelming majority of the data is a single class.
Typically **positive::positive/negative** class is the class that requires action
Negative class is the one where we exhibit default behavior (no action)
> very common to have a rare positive signal among a lot of negatives

**Precision** measures **how many of the things classified as positive were actually positive**: **$\frac{tp}{tp + fp}$ (true positive, false positive)**
> Max's preferred way of looking at model fit

**Recall** measures **how many of the positive class we missed (true positive, false negative)**: **$\frac{tp}{tp + fn}$**

Accuracy in machine learning is: **categorized correct / total categorized**

Recall and precision is often a **tradeoff**; it can never be perfect

**F1** score: used when both **precision** and **1::recall** are important.
> email Max if u ever figure this out lmfao

**Confidence score** indicates the probability that measures the degree of statistical certainty that the extracted result is detected correctly.

**Log loss**: measure of accuracy that penalizes overconfidence 

The **receiver-operating characteristic curve (ROC)** is the graph of **true positive rate vs false positive rate**. 
> as we change the threshold for classification, the model will begin to get more truve positives as well as more false positives
> ![](z_attachments/Pasted%20image%2020250401150637.png)


For the receiver-operating characteristic curve: 
The further the curve is to the **top left corner** the better the classification.
> ![](z_attachments/Pasted%20image%2020250401150637.png)

For the receiver-operating characteristic curve: 
The **diagonal** line is the **random** classifier
> ![](z_attachments/Pasted%20image%2020250401150637.png)

The **left-top border** line is the **perfect** classifier
> ![](z_attachments/Pasted%20image%2020250401150637.png)

For the receiver-operating characteristic curve: 
**Grid search** is the simplest algorithm for hyperparameter tuning. 
	![](z_attachments/Pasted%20image%2020250401152910.png)
***

$\mathrm{Accuracy}=**\frac{\text{correct classifications}}{\text{total classifications}}**=**2::\frac{TP+TN}{TP+TN+FP+FN}**$


$\text{Recall (or **TPR**)}=**\frac{\text{correctly classified actual positives}}{\text{all actual positives}}**=**2::\frac{TP}{TP+FN}**$

$\mathrm{FPR}=**\frac{\text{incorrectly classified actual negatives}}{\text{all actual negatives}}**=**2::\frac{FP}{FP+TN}**$

$**\mathrm{Precision}**=**\frac{\text{correctly classified actual positives}}{\text{everything classified as positive}}**=**2::\frac{TP}{TP+FP}**$

***

There is a risk of overfitting on the test set because **the parameters can be tweaked until the estimator performs optimally**. Thus create a **validation** set
+
However, this reduces the number of samples for training. Solution: **cross validation**


***
