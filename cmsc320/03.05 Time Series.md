Mathematically, a **time series** is a series of data points **indexed (or listed or graphed) in time order**.
We will only consider a series to be a time series if **past observations contain predictive value for the future**.

**Autocorrelation** is the correlation of a time series and a delayed version over time.

**Partial autocorrelation** is the autocorrelation between $y_t$ and $y_{t–h}$ after the removal of any linear dependence on $y_1, y_2, \cdots, y_{t–h+1}$
> [Time Series Talk : Autocorrelation and Partial Autocorrelation](https://www.youtube.com/watch?v=DeORzP0go5I)
> [Difference between autocorrelation and partial autocorrelation](https://stats.stackexchange.com/questions/483383/difference-between-autocorrelation-and-partial-autocorrelation#:~:text=Autocorrelation%20between%20X%20and%20Z,on%20X%20coming%20through%20Y.)

For missing data in a time series, **linear imputation** is probably good enough.
> ![](z_attachments/Pasted%20image%2020250501141625.png)

Time Series Components
- **Noise: changes based on variables invisible to us**
- **1::Trend: the long term direction of the time series**
- **1::Seasonality: cyclic behavior**
> noise: observations are randomly shifted from the trend
> noise is typically gaussian (Gaussian noise, also known as normal or additive white Gaussian noise (AWGN), is a type of random noise that follows a normal distribution with a mean of zero and a specific standard deviation)

A time series is **stationary** if its **mean and variance remain constant over time**.
> indicates a lack of trend

The **Dickey–Fuller** test tests the null hypothesis that **a unit root is present (the data is non-stationary) in an autoregressive (AR) time series model**. 

STL is an acronym for “**Seasonal and Trend decomposition** using Loess”
+
Decompose time series to find:
- **1::The overall trend of your series**
- **1::How it cycles**
but probably not both at the same time
> How?
> Model the trend, then subtract that out from the time series
> What you have left should be seasonality and noise

A **simple moving average (SMA)** is a calculation that takes the arithmetic mean of values over a specific time period in the past.

**Exponential smoothing** is a forecasting method that calculates a weighted average of past data points, **giving more weight to recent data points**.
> ![|300](z_attachments/Pasted%20image%2020250501142822.png)


An **autoregressive** model is when we regress a value from a time series on previous values from that same time series.
> For example, $y_t$ regressed on $y_{t-1}$ uses the previous value of $y$, called a lagged value, to predict the current value of $y$

A **moving-average** model is when we model a time series based on the linear combination of past error terms (shocks).  
> For example, \( y_t = \mu + \epsilon_t + \theta_1 \epsilon_{t-1} \) uses the previous shock (\(\epsilon_{t-1}\)) to help explain the current value of \( y \).  

An **ARIMA** model combines autoregression (AR), differencing (I), and moving averages (MA) to model non-stationary time series.  
> For example, ARIMA(1,1,1) means:  
> - **AR(1)**: \( y_t \) depends on \( y_{t-1} \)  
> - **I(1)**: First differencing to remove trend  
> - **MA(1)**: Includes one lagged error term

A **SARIMA** model extends ARIMA by adding **seasonal terms to capture repeating patterns**.  
> For example, SARIMA(1,1,1)(1,1,1,4) means:  
> - Non-seasonal part: ARIMA(1,1,1)  
> - Seasonal part:  
>   - **AR(1)** on seasonal lags (e.g., \( y_{t-4} \))  
>   - **MA(1)** on seasonal errors (e.g., \( \epsilon_{t-4} \))  
>   - Seasonal differencing (lag 4)  

A Long short-term memory (LSTM) is useful for learning **long-term dependencies**.
> [Understanding LSTM Networks -- colah's blog](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

**Recurrent neural networks (RNNs)** are a class of artificial neural networks designed for processing **sequential** data, such as text, speech, and time series,[1] where the **2::order** of elements is important.
> Unlike [feedforward neural networks](https://en.wikipedia.org/wiki/Feedforward_neural_network "Feedforward neural network"), which process inputs independently, RNNs utilize recurrent connections, where the output of a neuron at one time step is fed back as input to the network at the next time step. This enables RNNs to capture temporal dependencies and patterns within sequences.

A typical LSTM cell takes:
- **one time step input sensor $x$**
- **1::cell memory $c$** 
- **1::hidden state $h$**
passes them through:
- **forget** gate 
- input gate
and outputs through an output gate:
- **cell memory**
- **3::hidden state**
> cell memory and hidden state can be initialized to zero at the beginning
> forget gate decides what information is relevant to keep from the previous time step
> ![](z_attachments/Pasted%20image%2020250501162020.png)

To use a LSTM on time series:
- pass the LSTM over the whole time series, moving the cell and hidden state forward
- use the prediction from the **last** cell

Method of evaluating time series:
- **1::sliding window**
- **1::train first 90%, test last 10%**

***
