# perceptrons

Perceptron should be used when a dataset is **linearly separable**
	![|400](z_attachments/Pasted%20image%2020250406155956.png)
	[Perceptron](https://www.youtube.com/watch?v=4Gac5I64LM4)

Perceptron parameters:
$\omega$: number of **dimensions** (coefficients + 1 for constant term)
$\nu$: **learning rate** (how fast the perceptron converges to a line separating two classes; tradeoff between **2::speed** and **2::mistakes**)
+
Perceptron inputs: 
$x=\left(\begin{matrix}1\\x_{1}\\x_{2}\end{matrix}\right)$
	[Perceptron](https://www.youtube.com/watch?v=4Gac5I64LM4)

 
Update step of perceptron:
for each **misclassification**:
$w_{i}=w_{i}+\nu dx_{i}$, where $d=**\begin{cases}1 & \textrm{if should be upper}\\ -1 & \textrm{if should be lower} \end{cases}**$
	[Perceptron](https://www.youtube.com/watch?v=4Gac5I64LM4)


# neural networks

A neural network consists of an input layer, an output layer, and **hidden layers**.

A node's **activation** is calculated: **$\sum_i^Nw_ia_i$**, which measures **how closely inputs fit the criteria of the current node**
+
Weighted sums can produce any real number. Thus a common thing to do to scale things down is to apply the **sigmoid (logistic curve) (although recently the rectified linear unit is more common $\max(0,ax)=a\max(0,x)\text{ for }a\geq0$)** function to the results.
	this example where negative weights are associated with red looks for a horizontal line around the top of the image
	![|200](z_attachments/Pasted%20image%2020250406162123.png)
	[But what is a neural network? \| Deep learning chapter 1](https://www.youtube.com/watch?v=aircAruvnKk)

During the activation calculation $\operatorname*{\sigma}(w_1a_1+w_2{a_2}+w_3a_3+\cdots+w_na_n)$ in a neural network, a **bias for inactivity** can be applied through the addition of a constant term.
	![](z_attachments/Pasted%20image%2020250406162841.png)

**Backpropagation** is the algorithm used to **train neural networks**.
	[Backpropagation, intuitively \| DL3](https://www.youtube.com/watch?v=Ilg3gGewQ5U)

We want to alter weights in a neural network to give the biggest bang for the buck. Thus the **gradient** operator is used.
	[Backpropagation, intuitively \| DL3](https://www.youtube.com/watch?v=Ilg3gGewQ5U)

For a particular input set, backpropagation starts from the last layer and notes **which activations should increase (closer to 1) and which should decrease (closer to 0) and by what amount.** 
These "nudges" in the second to last layer that each neuron in the last layer wants are **summed** and repeated for the last layer.
	loss function $L=(predicted-actual)^2$
	$\frac{\delta L}{\delta w_i}$
	[Backpropagation, intuitively \| DL3](https://www.youtube.com/watch?v=Ilg3gGewQ5U)
	this is slow! stochastic gradient descent (splitting training data into smaller buckets) may be used to approximate the gradient
