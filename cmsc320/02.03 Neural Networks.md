**Perceptron** algorithm:
1. Start with all zeros weight vector and initialize t to 1. Automatically scale all examples to have (Euclidean) length 1, since this doesn't affect which side of the plane they are on
2. Given example $x$, predict positive iff $wt \cdot x > 0$
3. On a mistake, update as follows:
mistake on positive:$:\mathrm{w}_{t+1}\leftarrow\mathrm{w}_{t}+(\mathrm{x}^{*}\alpha)$
mistake on negative: $\mathrm{W}_{\mathrm{t}+1}\leftarrow\mathrm{W}_{\mathrm{t}}-(\mathrm{x}^*\mathrm{a})$

what the fuckkkkk i don'tu nderstand this shit 

Multi-layer neural networks:
- Each computational unit, or neuron, is a node in a directed acyclic graph
- Neurons can have activation values, representing how activated one particular neuron is. ● Neurons can pass their activation values on to their neighbors, based on the weight of the connection between them. ● A multi-layer neural network usually has an input layer and an output layer.

Sigmoid function scales it down

Back propagation: derivatives 

Weights with high gradient strongly contribute to incorect classifciation 

not expected to know the math--follow up on this fuck me i fucking hate math holy shit

Neural networks: are finicky

watch 3b1b fuckkkkkk meeeeeeeeee

naive bayes
> often used in NLP
> works best if variables are conditionally independent

Support vector machines divide data based on the maximum margin hyperplane. 
If the data is not linearly separable , it uses cool math to introduce new dimensions that slice the data

Random forests: probably the most powerful classifier we have right now
> creates n different decision trees, each trained on a subset of the data to avoid overfitting
> Each decision tree votes
> If a decision tree is overfit or has locked on to a weird correlation, it gets outvoted
> Not great for super high-dimensional data
> ![](z_attachments/Pasted%20image%2020250403142810.png)



Scikit learn has a classifier super object with the methods:
- **`fit(data, labels)`**: A function that trains the model
- **`predict(unlabelled_data)`**: Takes in unlabeled data and produces an array of labels


How Max debugs dataframes:
- A dataframe of random data, completely uncorrelated from my target
> should be no better than random guessing

How Max debugs dataframes:
- A dataframe that has exactly the same data as my regular data, except an extra column that is my target
> what does he mean by 100% target?
> should get 100%

How Max debugs dataframes:
- A dataframe where I overwrite the target to be a really simple function of my existing data
> eg it should pick up on doubling age producing doubling age 

To deal with class imbalance: 
- **use the `class_weight` parameter in Sklearn**
- **create multiple examples of negative features with slight differences**
- equalize your training set 
